{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5d4959a9",
      "metadata": {},
      "source": [
        "# SAM2 Module Names (for PEFT)\n",
        "Enumerate module names so you can decide which layers to target with LoRA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "31bebbdd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model type: <class 'sam2.modeling.sam2_base.SAM2Base'>\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "ROOT = Path.cwd().parent\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(ROOT))\n",
        "SAM2_ROOT = ROOT.parent / \"sam2\"\n",
        "if SAM2_ROOT.exists() and str(SAM2_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(SAM2_ROOT))\n",
        "\n",
        "from sam2.build_sam import build_sam2\n",
        "from debug_tests.debug_test import MAIN_DIR, _load_constants\n",
        "\n",
        "constants = _load_constants()\n",
        "model = build_sam2(constants[\"model_cfg\"], constants[\"checkpoint\"], device=\"cpu\")\n",
        "print(\"Model type:\", type(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ab7605bb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sam_mask_decoder -> MaskDecoder\n",
            "sam_mask_decoder.transformer -> TwoWayTransformer\n",
            "sam_mask_decoder.transformer.layers -> ModuleList\n",
            "sam_mask_decoder.transformer.layers.0 -> TwoWayAttentionBlock\n",
            "sam_mask_decoder.transformer.layers.0.self_attn -> Attention\n",
            "sam_mask_decoder.transformer.layers.0.self_attn.q_proj -> Linear\n",
            "sam_mask_decoder.transformer.layers.0.self_attn.k_proj -> Linear\n",
            "sam_mask_decoder.transformer.layers.0.self_attn.v_proj -> Linear\n",
            "sam_mask_decoder.transformer.layers.0.self_attn.out_proj -> Linear\n",
            "sam_mask_decoder.transformer.layers.0.norm1 -> LayerNorm\n",
            "sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image -> Attention\n",
            "sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj -> Linear\n",
            "sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj -> Linear\n",
            "sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj -> Linear\n",
            "sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj -> Linear\n",
            "sam_mask_decoder.transformer.layers.0.norm2 -> LayerNorm\n",
            "sam_mask_decoder.transformer.layers.0.mlp -> MLP\n",
            "sam_mask_decoder.transformer.layers.0.mlp.layers -> ModuleList\n",
            "sam_mask_decoder.transformer.layers.0.mlp.layers.0 -> Linear\n",
            "sam_mask_decoder.transformer.layers.0.mlp.layers.1 -> Linear\n",
            "sam_mask_decoder.transformer.layers.0.mlp.act -> ReLU\n",
            "sam_mask_decoder.transformer.layers.0.norm3 -> LayerNorm\n",
            "sam_mask_decoder.transformer.layers.0.norm4 -> LayerNorm\n",
            "sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token -> Attention\n",
            "sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj -> Linear\n",
            "sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj -> Linear\n",
            "sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj -> Linear\n",
            "sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj -> Linear\n",
            "sam_mask_decoder.transformer.layers.1 -> TwoWayAttentionBlock\n",
            "sam_mask_decoder.transformer.layers.1.self_attn -> Attention\n",
            "sam_mask_decoder.transformer.layers.1.self_attn.q_proj -> Linear\n",
            "sam_mask_decoder.transformer.layers.1.self_attn.k_proj -> Linear\n",
            "sam_mask_decoder.transformer.layers.1.self_attn.v_proj -> Linear\n",
            "sam_mask_decoder.transformer.layers.1.self_attn.out_proj -> Linear\n",
            "sam_mask_decoder.transformer.layers.1.norm1 -> LayerNorm\n",
            "sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image -> Attention\n",
            "sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj -> Linear\n",
            "sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj -> Linear\n",
            "sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj -> Linear\n",
            "sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj -> Linear\n",
            "sam_mask_decoder.transformer.layers.1.norm2 -> LayerNorm\n",
            "sam_mask_decoder.transformer.layers.1.mlp -> MLP\n",
            "sam_mask_decoder.transformer.layers.1.mlp.layers -> ModuleList\n",
            "sam_mask_decoder.transformer.layers.1.mlp.layers.0 -> Linear\n",
            "sam_mask_decoder.transformer.layers.1.mlp.layers.1 -> Linear\n",
            "sam_mask_decoder.transformer.layers.1.mlp.act -> ReLU\n",
            "sam_mask_decoder.transformer.layers.1.norm3 -> LayerNorm\n",
            "sam_mask_decoder.transformer.layers.1.norm4 -> LayerNorm\n",
            "sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token -> Attention\n",
            "sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj -> Linear\n",
            "sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj -> Linear\n",
            "sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj -> Linear\n",
            "sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj -> Linear\n",
            "sam_mask_decoder.transformer.final_attn_token_to_image -> Attention\n",
            "sam_mask_decoder.transformer.final_attn_token_to_image.q_proj -> Linear\n",
            "sam_mask_decoder.transformer.final_attn_token_to_image.k_proj -> Linear\n",
            "sam_mask_decoder.transformer.final_attn_token_to_image.v_proj -> Linear\n",
            "sam_mask_decoder.transformer.final_attn_token_to_image.out_proj -> Linear\n",
            "sam_mask_decoder.transformer.norm_final_attn -> LayerNorm\n",
            "sam_mask_decoder.iou_token -> Embedding\n",
            "sam_mask_decoder.mask_tokens -> Embedding\n",
            "sam_mask_decoder.obj_score_token -> Embedding\n",
            "sam_mask_decoder.output_upscaling -> Sequential\n",
            "sam_mask_decoder.output_upscaling.0 -> ConvTranspose2d\n",
            "sam_mask_decoder.output_upscaling.1 -> LayerNorm2d\n",
            "sam_mask_decoder.output_upscaling.2 -> GELU\n",
            "sam_mask_decoder.output_upscaling.3 -> ConvTranspose2d\n",
            "sam_mask_decoder.output_upscaling.4 -> GELU\n",
            "sam_mask_decoder.conv_s0 -> Conv2d\n",
            "sam_mask_decoder.conv_s1 -> Conv2d\n",
            "sam_mask_decoder.output_hypernetworks_mlps -> ModuleList\n",
            "sam_mask_decoder.output_hypernetworks_mlps.0 -> MLP\n",
            "sam_mask_decoder.output_hypernetworks_mlps.0.layers -> ModuleList\n",
            "sam_mask_decoder.output_hypernetworks_mlps.0.layers.0 -> Linear\n",
            "sam_mask_decoder.output_hypernetworks_mlps.0.layers.1 -> Linear\n",
            "sam_mask_decoder.output_hypernetworks_mlps.0.layers.2 -> Linear\n",
            "sam_mask_decoder.output_hypernetworks_mlps.0.act -> ReLU\n",
            "sam_mask_decoder.output_hypernetworks_mlps.1 -> MLP\n",
            "sam_mask_decoder.output_hypernetworks_mlps.1.layers -> ModuleList\n",
            "sam_mask_decoder.output_hypernetworks_mlps.1.layers.0 -> Linear\n",
            "sam_mask_decoder.output_hypernetworks_mlps.1.layers.1 -> Linear\n",
            "sam_mask_decoder.output_hypernetworks_mlps.1.layers.2 -> Linear\n",
            "sam_mask_decoder.output_hypernetworks_mlps.1.act -> ReLU\n",
            "sam_mask_decoder.output_hypernetworks_mlps.2 -> MLP\n",
            "sam_mask_decoder.output_hypernetworks_mlps.2.layers -> ModuleList\n",
            "sam_mask_decoder.output_hypernetworks_mlps.2.layers.0 -> Linear\n",
            "sam_mask_decoder.output_hypernetworks_mlps.2.layers.1 -> Linear\n",
            "sam_mask_decoder.output_hypernetworks_mlps.2.layers.2 -> Linear\n",
            "sam_mask_decoder.output_hypernetworks_mlps.2.act -> ReLU\n",
            "sam_mask_decoder.output_hypernetworks_mlps.3 -> MLP\n",
            "sam_mask_decoder.output_hypernetworks_mlps.3.layers -> ModuleList\n",
            "sam_mask_decoder.output_hypernetworks_mlps.3.layers.0 -> Linear\n",
            "sam_mask_decoder.output_hypernetworks_mlps.3.layers.1 -> Linear\n",
            "sam_mask_decoder.output_hypernetworks_mlps.3.layers.2 -> Linear\n",
            "sam_mask_decoder.output_hypernetworks_mlps.3.act -> ReLU\n",
            "sam_mask_decoder.iou_prediction_head -> MLP\n",
            "sam_mask_decoder.iou_prediction_head.layers -> ModuleList\n",
            "sam_mask_decoder.iou_prediction_head.layers.0 -> Linear\n",
            "sam_mask_decoder.iou_prediction_head.layers.1 -> Linear\n",
            "sam_mask_decoder.iou_prediction_head.layers.2 -> Linear\n",
            "sam_mask_decoder.iou_prediction_head.act -> ReLU\n",
            "sam_mask_decoder.pred_obj_score_head -> MLP\n",
            "sam_mask_decoder.pred_obj_score_head.layers -> ModuleList\n",
            "sam_mask_decoder.pred_obj_score_head.layers.0 -> Linear\n",
            "sam_mask_decoder.pred_obj_score_head.layers.1 -> Linear\n",
            "sam_mask_decoder.pred_obj_score_head.layers.2 -> Linear\n",
            "sam_mask_decoder.pred_obj_score_head.act -> ReLU\n"
          ]
        }
      ],
      "source": [
        "# Filtered: likely decoder blocks / attention / MLP\n",
        "keywords = [\"sam_mask_decoder\"]\n",
        "for name, module in model.named_modules():\n",
        "    if any(k in name.lower() for k in keywords):\n",
        "        print(name, \"->\", module.__class__.__name__)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sam2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
